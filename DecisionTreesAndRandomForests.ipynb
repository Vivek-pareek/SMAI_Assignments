{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "1f8d80d535cfd832283e4e3a1095d2ce45fe6627336684f2622a1965babb2f1c"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit (windows store)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "DecisionTreesAndRandomForests.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vivek-pareek/SMAI_Assignments/blob/main/DecisionTreesAndRandomForests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0_wToNFHMN3"
      },
      "source": [
        "# **Decision Trees**\n",
        "\n",
        "The Wisconsin Breast Cancer Dataset(WBCD) can be found here(https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data)\n",
        "\n",
        "This dataset describes the characteristics of the cell nuclei of various patients with and without breast cancer. The task is to classify a decision tree to predict if a patient has a benign or a malignant tumour based on these features.\n",
        "\n",
        "Attribute Information:\n",
        "```\n",
        "#  Attribute                     Domain\n",
        "   -- -----------------------------------------\n",
        "   1. Sample code number            id number\n",
        "   2. Clump Thickness               1 - 10\n",
        "   3. Uniformity of Cell Size       1 - 10\n",
        "   4. Uniformity of Cell Shape      1 - 10\n",
        "   5. Marginal Adhesion             1 - 10\n",
        "   6. Single Epithelial Cell Size   1 - 10\n",
        "   7. Bare Nuclei                   1 - 10\n",
        "   8. Bland Chromatin               1 - 10\n",
        "   9. Normal Nucleoli               1 - 10\n",
        "  10. Mitoses                       1 - 10\n",
        "  11. Class:                        (2 for benign, 4 for malignant)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYdlWpUVHMOB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "553ffa55-e89d-49a5-d488-dc7c92e706e8"
      },
      "source": [
        "import pandas as pd\n",
        "headers = [\"ID\",\"CT\",\"UCSize\",\"UCShape\",\"MA\",\"SECSize\",\"BN\",\"BC\",\"NN\",\n",
        "           \"Mitoses\",\"Diagnosis\"]\n",
        "data = pd.read_csv('breast-cancer-wisconsin.data', na_values='?',    \n",
        "         header=None, index_col=['ID'], names = headers) \n",
        "data = data.reset_index(drop=True)\n",
        "data = data.fillna(0)\n",
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CT</th>\n",
              "      <th>UCSize</th>\n",
              "      <th>UCShape</th>\n",
              "      <th>MA</th>\n",
              "      <th>SECSize</th>\n",
              "      <th>BN</th>\n",
              "      <th>BC</th>\n",
              "      <th>NN</th>\n",
              "      <th>Mitoses</th>\n",
              "      <th>Diagnosis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>699.000000</td>\n",
              "      <td>699.000000</td>\n",
              "      <td>699.000000</td>\n",
              "      <td>699.000000</td>\n",
              "      <td>699.000000</td>\n",
              "      <td>699.000000</td>\n",
              "      <td>699.000000</td>\n",
              "      <td>699.000000</td>\n",
              "      <td>699.000000</td>\n",
              "      <td>699.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>4.417740</td>\n",
              "      <td>3.134478</td>\n",
              "      <td>3.207439</td>\n",
              "      <td>2.806867</td>\n",
              "      <td>3.216023</td>\n",
              "      <td>3.463519</td>\n",
              "      <td>3.437768</td>\n",
              "      <td>2.866953</td>\n",
              "      <td>1.589413</td>\n",
              "      <td>2.689557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.815741</td>\n",
              "      <td>3.051459</td>\n",
              "      <td>2.971913</td>\n",
              "      <td>2.855379</td>\n",
              "      <td>2.214300</td>\n",
              "      <td>3.640708</td>\n",
              "      <td>2.438364</td>\n",
              "      <td>3.053634</td>\n",
              "      <td>1.715078</td>\n",
              "      <td>0.951273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               CT      UCSize     UCShape  ...          NN     Mitoses   Diagnosis\n",
              "count  699.000000  699.000000  699.000000  ...  699.000000  699.000000  699.000000\n",
              "mean     4.417740    3.134478    3.207439  ...    2.866953    1.589413    2.689557\n",
              "std      2.815741    3.051459    2.971913  ...    3.053634    1.715078    0.951273\n",
              "min      1.000000    1.000000    1.000000  ...    1.000000    1.000000    2.000000\n",
              "25%      2.000000    1.000000    1.000000  ...    1.000000    1.000000    2.000000\n",
              "50%      4.000000    1.000000    1.000000  ...    1.000000    1.000000    2.000000\n",
              "75%      6.000000    5.000000    5.000000  ...    4.000000    1.000000    4.000000\n",
              "max     10.000000   10.000000   10.000000  ...   10.000000   10.000000    4.000000\n",
              "\n",
              "[8 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_gQq5qrHMOG"
      },
      "source": [
        "1. a) Implement a decision tree (you can use decision tree implementation from existing libraries)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6R3GmzBHMOH"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "import random\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ7N9m_mHMOJ"
      },
      "source": [
        "1. b) Train a decision tree object of the above class on the WBC dataset using misclassification rate, entropy and Gini as the splitting metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHFij6PaHMOJ"
      },
      "source": [
        "#We define decision trees using gini and entropy based methods\n",
        "decision_tree_entropy = DecisionTreeClassifier(criterion = 'entropy')\n",
        "decision_tree_gini = DecisionTreeClassifier(criterion = 'gini')\n",
        "\n",
        "#get the feature columns in the data\n",
        "feature_cols = headers[1 : -1]\n",
        "\n",
        "#get the output classes\n",
        "class_label = headers[-1]\n",
        "\n",
        "#use library function to split data into test and train set\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data[feature_cols], data[class_label], test_size = 0.2, random_state = 1)\n",
        "\n",
        "#fit the models to our training data\n",
        "\n",
        "decision_tree_entropy = decision_tree_entropy.fit(X_train, y_train)\n",
        "y_pred_entropy = decision_tree_entropy.predict(X_test)\n",
        "\n",
        "decision_tree_gini = decision_tree_gini.fit(X_train, y_train)\n",
        "y_pred_gini = decision_tree_gini.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXEjInvmHMOK"
      },
      "source": [
        "1. c) Report the accuracies in each of the above splitting metrics and give the best result. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49QZvmgNHMOL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "508c818e-2416-4adc-af56-b22ab05ddb52"
      },
      "source": [
        "print('Accuracy using gini index for splitting is {:.2f}'.format(\n",
        "    metrics.accuracy_score(y_test, y_pred_gini)))\n",
        "\n",
        "print('Accuracy using entropy for splitting is {:.2f}'.format(\n",
        "    metrics.accuracy_score(y_test, y_pred_entropy)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using gini index for splitting is 0.92\n",
            "Accuracy using entropy for splitting is 0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz_7nYxPHMON"
      },
      "source": [
        "1. d) Experiment with different approaches to decide when to terminate the tree (number of layers, purity measure, etc). Report and give explanations for all approaches. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCjgfgMK18LZ",
        "outputId": "7f4f228e-9227-4635-9be4-b1cb68320d98"
      },
      "source": [
        "'''\n",
        "In the following code we will try the models with pruning the depth from \n",
        "(max_depth - 1) upto 2. We also use only sqrt of the number of features\n",
        "at each split to determine the split\n",
        "'''\n",
        "\n",
        "# Method to prune decision trees at various depths and check accuracy score\n",
        "#By default uses sqrt of features at each split\n",
        "def experiment_depths(X_train, y_train, X_test, y_test, max_depth = None,\n",
        "                      criterion = 'gini', max_features = \"sqrt\"):\n",
        "  start_depth = 2\n",
        "\n",
        "  max_score, max_score_depth = 0, 0\n",
        "\n",
        "  for depth in range(start_depth, max_depth):\n",
        "    emperical_decision_tree = DecisionTreeClassifier(max_depth = depth, \n",
        "                                                     criterion = criterion,\n",
        "                                                     max_features = max_features)\n",
        "    emperical_decision_tree = emperical_decision_tree.fit(X_train, y_train)\n",
        "    y_pred = emperical_decision_tree.predict(X_test)\n",
        "    score = metrics.accuracy_score(y_test, y_pred)\n",
        "    if score > max_score:\n",
        "      max_score = score\n",
        "      max_score_depth = depth\n",
        "    print(\"\\naccuracy score for decision tree using {} at max depth {} is {:.4f}\".\n",
        "          format(criterion,\n",
        "                 emperical_decision_tree.tree_.max_depth,\n",
        "                 metrics.accuracy_score(y_test, y_pred)))\n",
        "    \n",
        "  return max_score, max_score_depth, criterion\n",
        "\n",
        "entropy_max_score, entropy_max_depth, criterion_1 = experiment_depths(X_train, \n",
        "                  y_train, X_test, y_test, max_depth = \n",
        "                  (decision_tree_entropy.tree_.max_depth), criterion = \n",
        "                  'entropy')\n",
        "\n",
        "gini_max_score, gini_max_depth, criterion_2 = experiment_depths(\n",
        "    X_train, y_train, X_test, y_test, max_depth = (\n",
        "        decision_tree_gini.tree_.max_depth), criterion = 'gini')\n",
        "\n",
        "if gini_max_score > entropy_max_score :\n",
        "  max_score = gini_max_score \n",
        "  print(\"\\n\\n The maximum accuracy in decision trees is {:.3f} after pruning at depth {} for criterion {}\".format(gini_max_score, gini_max_depth,\n",
        "                                         \"gini\"))\n",
        "else :\n",
        "  max_score = entropy_max_score \n",
        "  print(\"\\n\\n The maximum accuracy in decision trees is {:.3f} after pruning at depth {} for criterion {}\".format(entropy_max_score, entropy_max_depth,\"entropy\"))  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "accuracy score for decision tree using entropy at max depth 2 is 0.9214\n",
            "\n",
            "accuracy score for decision tree using entropy at max depth 3 is 0.9000\n",
            "\n",
            "accuracy score for decision tree using entropy at max depth 4 is 0.9643\n",
            "\n",
            "accuracy score for decision tree using entropy at max depth 5 is 0.9500\n",
            "\n",
            "accuracy score for decision tree using entropy at max depth 6 is 0.9357\n",
            "\n",
            "accuracy score for decision tree using entropy at max depth 7 is 0.9571\n",
            "\n",
            "accuracy score for decision tree using entropy at max depth 8 is 0.9357\n",
            "\n",
            "accuracy score for decision tree using gini at max depth 2 is 0.9643\n",
            "\n",
            "accuracy score for decision tree using gini at max depth 3 is 0.9429\n",
            "\n",
            "accuracy score for decision tree using gini at max depth 4 is 0.9429\n",
            "\n",
            "accuracy score for decision tree using gini at max depth 5 is 0.9571\n",
            "\n",
            "accuracy score for decision tree using gini at max depth 6 is 0.9500\n",
            "\n",
            "accuracy score for decision tree using gini at max depth 7 is 0.9143\n",
            "\n",
            "accuracy score for decision tree using gini at max depth 8 is 0.9357\n",
            "\n",
            "\n",
            " The maximum accuracy in decision trees is 0.964 after pruning at depth 4 for criterion entropy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up3DVetvAHjM"
      },
      "source": [
        "<u>explanation:</u>\n",
        "<br>\n",
        "<b>1.We perform depth pruning of the tree to avoid overfitting of the model to data, we can see from above output that the maximum accuracy occurs if we prune the tree earlier than the maximum depth.\n",
        "<br>\n",
        "<b>2.We use sqrt of features at each split. \n",
        "<br>\n",
        "<b>Both these methods reduce overfitting of our model to data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLRI0niJHMOP"
      },
      "source": [
        "class RandomForestBinaryClassifier():  \n",
        "  '''\n",
        "    Implementation of a simple random forest for binary classification.\n",
        "    Here we create a forest of decision trees with sample size equal\n",
        "    to the entire data size but each sample has data rows picked at random \n",
        "    with replacement\n",
        "  '''\n",
        "  \n",
        "  '''\n",
        "    max_depth = Maximum depth of decision trees in our forest\n",
        "    criterion = criterion to be used for splitting nodes\n",
        "    min_samples_split = No of min samples to be present to split a node\n",
        "    min_samples_leaf = Minimum number of samples to be present in leaf node\n",
        "    n_trees = No of trees in our forest\n",
        "    max_features = maximum features to be considered at each node split\n",
        "  '''\n",
        "\n",
        "  def __init__(self, max_depth = None, criterion = \"gini\", min_samples_split = 2,\n",
        "                   min_samples_leaf = 1, n_trees = 100, max_features = None):\n",
        "    self.max_depth = max_depth\n",
        "    self.criterion = criterion\n",
        "    self.min_samples_split = min_samples_split\n",
        "    self.min_samples_leaf = min_samples_leaf\n",
        "    self.n_trees = n_trees\n",
        "    self.decision_tree_list = None\n",
        "    self.max_features = max_features\n",
        "    self.label = None\n",
        "\n",
        "  def fit(self, X_train, y_train):\n",
        "    self.label = y_train.name\n",
        "    decision_tree_list = []\n",
        "\n",
        "    #we create the given number of trees for the forest \n",
        "    for i in range(self.n_trees):\n",
        "      sample_array = []\n",
        "      sample_class_array = []\n",
        "      \n",
        "      for i in range(X_train.shape[0]):\n",
        "        '''\n",
        "          we add data rows at random with replacement to our data.\n",
        "          Note that the size of sample is same as population. But the sample \n",
        "          has data rows at random with replacement, so we can have \n",
        "          duplicate rows in sample.\n",
        "        '''\n",
        "        index = random.randint(0, X_train.shape[0]-1)\n",
        "        sample_array.append(X_train.iloc[index])\n",
        "        sample_class_array.append(y_train.iloc[index])\n",
        "      sample_data = pd.DataFrame(sample_array, columns = headers[1:-1])\n",
        "      sample_data_class = pd.DataFrame(sample_class_array, \n",
        "                                       columns = [y_train.name])\n",
        "      decision_tree_classifier = DecisionTreeClassifier(\n",
        "          max_depth = self.max_depth, criterion = self.criterion, \n",
        "          min_samples_split = self.min_samples_split, \n",
        "          min_samples_leaf = self.min_samples_leaf,\n",
        "          max_features = self.max_features)\n",
        "      decision_tree_classifier = decision_tree_classifier.fit(\n",
        "          sample_data, sample_data_class)\n",
        "      decision_tree_list.append(decision_tree_classifier)\n",
        "\n",
        "    self.decision_tree_list = decision_tree_list\n",
        "    return self\n",
        "\n",
        "  '''\n",
        "    Right now we have hard coded predict to just work on our breast-cancer-\n",
        "    wisconsin data\n",
        "  '''\n",
        "  def predict(self, X_test):\n",
        "    results = []\n",
        "    for decision_tree in self.decision_tree_list:\n",
        "      results.append(decision_tree.predict(X_test))\n",
        "    final_result = []\n",
        "    results = np.asarray(results).transpose(1,0)\n",
        "    for result in results:\n",
        "      count_2, count_4 = 0 , 0\n",
        "      for data in result:\n",
        "        if data == 2:\n",
        "          count_2 += 1\n",
        "        else :\n",
        "          count_4 += 1\n",
        "      final_result.append(2 if count_2 >= count_4\n",
        "                          else 4)\n",
        "    return pd.DataFrame(final_result, columns = [self.label])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWAN_wWXHMOQ"
      },
      "source": [
        "2. What is boosting, bagging and  stacking?\n",
        "Which class does random forests belong to and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnO5uqHlHMOR"
      },
      "source": [
        "Answer: \n",
        "\n",
        "These 3 techniques have one thing in common that they all in some way aggregate \n",
        "various ML models in a deterministic way to perform prediction or other tasks.\n",
        "<br>\n",
        "<br>\n",
        "<u>1.Bagging</u> : Generate additional data for your model using repetetion i.e\n",
        "random choices with replacement. This helps to decrease variance and helps in tuning our model. It is parallel in nature.(We are not implementing it parallely here)\n",
        "<br>\n",
        "<br>\n",
        "<u>2.Boosting</u> : We create subsets of original data and aggregate various \n",
        "average performing models to create our final model. It is different from\n",
        "bagging as the subsets are not at random, one subset depends on the subset used for previous models i.e it is sequential.\n",
        "<br>\n",
        "<br>\n",
        "<u>3.Stacking</u> : It is similar to boosting but uses another model to estimate the input with the output.\n",
        "\n",
        "<br>\n",
        "Random forests belong to the class of bagging because we take a number of \n",
        "decision trees and then to each tree we give a sample which is created from \n",
        "originial data at random with replacement and then the results are aggregated\n",
        "to give the final result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pihvGbqLHMOS"
      },
      "source": [
        "3. Implement random forest algorithm using different decision trees . "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXdPP2aIHMOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "723a5624-3428-4c5c-f493-a45b86e4b88b"
      },
      "source": [
        "'''\n",
        "*****\n",
        "\n",
        "Note: We have hardcoded our random forest model to work on the breast cancer data\n",
        "that we are using.Also we have developed it only for binary classification. \n",
        "\n",
        "ToDo: Make the random forest implementation generic. For now we use hardcoded \n",
        "implementation\n",
        "\n",
        "'''\n",
        "model_1 = RandomForestBinaryClassifier(max_features = \"sqrt\",\n",
        "                                     n_trees = 128)\n",
        "\n",
        "model_1 = model_1.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model_1.predict(X_test)\n",
        "\n",
        "print(\"Accuracy using random forest of decision trees using criterion gini is {:.3f}\".\n",
        "      format(metrics.accuracy_score(y_test, y_pred)))\n",
        "\n",
        "model_2 = RandomForestBinaryClassifier(max_features = \"sqrt\",\n",
        "                                     n_trees = 128, criterion = \"entropy\")\n",
        "\n",
        "model_2 = model_2.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model_2.predict(X_test)\n",
        "\n",
        "print(\"Accuracy using random forest of decision trees using criterion entropy is {:.3f}\".\n",
        "      format(metrics.accuracy_score(y_test, y_pred)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using random forest of decision trees using criterion gini is 0.964\n",
            "Accuracy using random forest of decision trees using criterion entropy is 0.971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJOn5nNZHMOU"
      },
      "source": [
        "4. Report the accuracies obtained after using the Random forest algorithm and compare it with the best accuracies obtained with the decision trees. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHqVyXVATFxS"
      },
      "source": [
        "<br>\n",
        "<b>1.Best accuracy we obtained in decision trees was 0.964. With Random forest we obtain an accuracy of 0.971 with criterion being entropy. Random forest performs better than decision tree\n",
        "in this case which is as expected by us.\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj-vNvsYHMOX"
      },
      "source": [
        "5. Submit your solution as a separate pdf in the final zip file of your submission\n",
        "\n",
        "\n",
        "Compute a decision tree with the goal to predict the food review based on its smell, taste and portion size.\n",
        "\n",
        "(a) Compute the entropy of each rule in the first stage.\n",
        "\n",
        "(b) Show the final decision tree. Clearly draw it.\n",
        "\n",
        "Submit a handwritten response. Clearly show all the steps.\n",
        "\n"
      ]
    }
  ]
}